{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09a88042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import nltk\n",
    "import string\n",
    "import pymorphy2\n",
    "import codecs\n",
    "\n",
    "class PrepareNew():\n",
    "  def __init__(self):\n",
    "    self.morph = pymorphy2.MorphAnalyzer()\n",
    "    self.tokenizer = nltk.WordPunctTokenizer()\n",
    "    self.stopwords = set(line.strip() for line in codecs.open('../rus_stopwords.txt', \"r\", \"utf_8_sig\").readlines())\n",
    "\n",
    "  def prepare_corp(self, news_list: List[str]):\n",
    "    return [self.newstext2token(news_text) for news_text in news_list]\n",
    "\n",
    "  def newstext2token(self, news_text: str):\n",
    "      tokens = self.tokenizer.tokenize(news_text.lower())\n",
    "      tokens_with_no_punct = [self.morph.parse(w)[0].normal_form for w in tokens if all(c not in string.punctuation for c in w)]\n",
    "      tokens_base_forms = [w for w in tokens_with_no_punct if w not in self.stopwords and w.isalpha()]\n",
    "      tokens_last = [w for w in tokens_base_forms if len(w)>1]\n",
    "      return tokens_last\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01f97afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = [line.strip() for line in codecs.open('../ved.txt', \"r\", \"utf_8_sig\").readlines() if line.strip()!=\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "576ff655",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = raw_data[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66afd1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_corp = PrepareNew().prepare_corp(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82195a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Phrases\n",
    "bigram_transformer = Phrases(my_corp)\n",
    "model = Word2Vec(bigram_transformer[my_corp], window=5, min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "202196e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.word2vec.Word2Vec"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d32f68b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('падать', 0.9817359447479248),\n",
       " ('цена_продовольствие', 0.9777151346206665),\n",
       " ('следующий_десятилетие', 0.9736192226409912),\n",
       " ('чистый_отток', 0.9734081029891968),\n",
       " ('оставаться_низкий', 0.9733332991600037),\n",
       " ('существенно_увеличиться', 0.973079264163971),\n",
       " ('сильно', 0.9725044369697571),\n",
       " ('доходность_облигация', 0.9714182615280151),\n",
       " ('говориться_обзор', 0.9708289504051208),\n",
       " ('снижаться', 0.9706270098686218)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('рубль', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0acb702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def text2vec(tokens, embeddings, dim=100):\n",
    "    \"\"\"\n",
    "        question: токены\n",
    "        embeddings: w2v модель\n",
    "        dim: размер любого вектора в нашем представлении\n",
    "        \n",
    "        return: векторное представление для вопроса\n",
    "    \"\"\"\n",
    "\n",
    "    relevant=0\n",
    "    words_vecs=np.zeros((dim,))\n",
    "    for word in tokens:\n",
    "      if word in embeddings.wv:\n",
    "        words_vecs+=embeddings.wv[word]\n",
    "        relevant+=1\n",
    "\n",
    "    if relevant:\n",
    "      words_vecs/=relevant\n",
    "    return words_vecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c4c5c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3336 3336\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "\n",
    "print(len(raw_data), len(my_corp))\n",
    "\n",
    "df['title'] = raw_data\n",
    "df['tokens'] = my_corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d4b996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vectors = np.array([text2vec(tokens, model) for tokens in my_corp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b42a01ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3336, 100)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2165d1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 121 nearest neighbors...\n",
      "[t-SNE] Indexed 3336 samples in 0.001s...\n",
      "[t-SNE] Computed neighbors for 3336 samples in 0.491s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 3336\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 3336\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 3336\n",
      "[t-SNE] Computed conditional probabilities for sample 3336 / 3336\n",
      "[t-SNE] Mean sigma: 0.253937\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 72.496048\n",
      "[t-SNE] KL divergence after 300 iterations: 1.662562\n",
      "t-SNE done! Time elapsed: 3.8178114891052246 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(data_vectors)\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229b8e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 + Spark 3",
   "language": "python",
   "name": "siriusenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
