{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "described-closing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymorphy2 in /opt/conda/lib/python3.8/site-packages (0.9.1)\r\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from pymorphy2) (0.7.2)\r\n",
      "Requirement already satisfied: docopt>=0.6 in /opt/conda/lib/python3.8/site-packages (from pymorphy2) (0.6.2)\r\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /opt/conda/lib/python3.8/site-packages (from pymorphy2) (2.4.417127.4579844)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "compound-watch",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import nltk\n",
    "import string\n",
    "import pymorphy2\n",
    "import codecs\n",
    "\n",
    "class PrepareForTopics():\n",
    "  def __init__(self):\n",
    "    self.morph = pymorphy2.MorphAnalyzer()\n",
    "    self.tokenizer = nltk.WordPunctTokenizer()\n",
    "    self.stopwords = set(line.strip() for line in codecs.open('rus_stopwords.txt', \"r\", \"utf_8_sig\").readlines())\n",
    "\n",
    "  def prepare_corp(self, news_list: List[str]):\n",
    "    return [self.newstext2token(news_text) for news_text in news_list]\n",
    "\n",
    "  def newstext2token(self, news_text: str):\n",
    "      tokens = self.tokenizer.tokenize(news_text.lower())\n",
    "      tokens_with_no_punct = [self.morph.parse(w)[0].normal_form for w in tokens if all(c not in string.punctuation for c in w)]\n",
    "      tokens_base_forms = [w for w in tokens_with_no_punct if w not in self.stopwords and w.isalpha()]\n",
    "      tokens_long = [w for w in tokens_base_forms if len(w)>1]\n",
    "      tokens_last = list(filter(lambda w: self.morph.parse(w)[0].tag.POS in ['ADJF', 'NOUN'], tokens_long))\n",
    "      return tokens_last\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "private-precipitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "raw_data = [re.sub(\"Данное сообщение (материал) создано и (или) распространено иностранным средством массовой информации, выполняющим функции иностранного агента, и (или) российским юридическим лицом, выполняющим функции иностранного агента. \\n\", '', line.strip()) for line in codecs.open('emb_dataset.txt', \"r\", \"utf_8_sig\").readlines() if line.strip()!=\"\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "written-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = PrepareForTopics().prepare_corp(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "marked-pacific",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['газохранилище', 'казань', 'улица', 'западный', 'взрыв', 'мчс', 'следственный', 'комитет', 'россия', 'татарстан', 'взрыв', 'газовый', 'заправка', 'уголовный', 'дело', 'нарушение', 'правило', 'безопасность', 'взрывоопасный', 'объект', 'ст', 'ук', 'рф'], ['результат', 'хлопок', 'газозаправочный', 'станция', 'рабочий', 'количество', 'факт', 'следственный', 'орган', 'республика', 'татарстан', 'уголовный', 'дело', 'пресс', 'релиз', 'ведомство']]\n"
     ]
    }
   ],
   "source": [
    "print(processed_docs[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "adjustable-shoot",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512487"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "manufactured-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create a dictionary from 'processed_docs' containing the number of times a word appears \n",
    "in the training set using gensim.corpora.Dictionary and call it 'dictionary'\n",
    "'''\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "dictionary = Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "vocal-lithuania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 безопасность\n",
      "1 взрыв\n",
      "2 взрывоопасный\n",
      "3 газовый\n",
      "4 газохранилище\n",
      "5 дело\n",
      "6 западный\n",
      "7 заправка\n",
      "8 казань\n",
      "9 комитет\n",
      "10 мчс\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Checking dictionary created\n",
    "'''\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "requested-holocaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "OPTIONAL STEP\n",
    "Remove very rare and very common words:\n",
    "\n",
    "- words appearing less than 15 times\n",
    "- words appearing in more than 10% of all documents\n",
    "'''\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.1, keep_n= 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "loving-sender",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create the Bag-of-words model for each document i.e for each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to 'bow_corpus'\n",
    "'''\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "experimental-clothing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 79 (\"сообщение\") appears 1 time.\n",
      "Word 82 (\"страна\") appears 1 time.\n",
      "Word 198 (\"больший\") appears 2 time.\n",
      "Word 202 (\"мир\") appears 1 time.\n",
      "Word 203 (\"надпись\") appears 1 time.\n",
      "Word 204 (\"название\") appears 1 time.\n",
      "Word 206 (\"самый\") appears 2 time.\n",
      "Word 210 (\"граффити\") appears 2 time.\n",
      "Word 211 (\"искусство\") appears 1 time.\n",
      "Word 212 (\"мировой\") appears 1 time.\n",
      "Word 213 (\"патриотический\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preview BOW for our sample preprocessed document\n",
    "'''\n",
    "document_num = 20\n",
    "bow_doc_x = bow_corpus[document_num]\n",
    "\n",
    "for i in range(len(bow_doc_x)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0], \n",
    "                                                     dictionary[bow_doc_x[i][0]], \n",
    "                                                     bow_doc_x[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-toolbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaMulticore\n",
    "lda_model =  LdaMulticore(bow_corpus, \n",
    "                                   num_topics = 8, \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 10,\n",
    "                                   workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-gates",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "'''\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "temp_file = datapath(\"LDAmodel\")\n",
    "lda_model.save(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-smart",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "print(morph.parse(\"произойти\")[0].tag.POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-funds",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
